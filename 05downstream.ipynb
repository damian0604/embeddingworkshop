{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond counting words: Working with word embeddings\n",
    "\n",
    "Workshop by Damian Trilling\n",
    "\n",
    "This notebook illustrates how we can use embeddings in Machine Learning tasks.\n",
    "\n",
    "As always, we first import neccesary modules. We also get our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install embeddingvectorizer    # you need to install this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised text classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import eli5\n",
    "from nltk.sentiment import vader\n",
    "\n",
    "from embeddingvectorizer import EmbeddingCountVectorizer, EmbeddingTfidfVectorizer\n",
    "import embeddingvectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "# general\n",
    "import numpy as np\n",
    "import re\n",
    "# word embedding stuff\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "\n",
    "# data\n",
    "from courseutils import get_review_data\n",
    "\n",
    "# lets get more output\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached file reviewdata.pickle.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:56:15,329 : INFO : loading Word2Vec object from mymodel\n",
      "2021-04-08 14:56:15,508 : INFO : loading wv recursively from mymodel.wv.* with mmap=None\n",
      "2021-04-08 14:56:15,509 : INFO : loading vectors from mymodel.wv.vectors.npy with mmap=None\n",
      "2021-04-08 14:56:15,578 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-04-08 14:56:15,579 : INFO : loading vocabulary recursively from mymodel.vocabulary.* with mmap=None\n",
      "2021-04-08 14:56:15,580 : INFO : loading trainables recursively from mymodel.trainables.* with mmap=None\n",
      "2021-04-08 14:56:15,581 : INFO : loading syn1neg from mymodel.trainables.syn1neg.npy with mmap=None\n",
      "2021-04-08 14:56:15,601 : INFO : setting ignored attribute cum_table to None\n",
      "2021-04-08 14:56:15,602 : INFO : loaded mymodel\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "reviews_train, reviews_test, y_train, y_test = get_review_data()\n",
    "\n",
    "reviews_train, y_train = shuffle(reviews_train, y_train, random_state=42)\n",
    "reviews_test, y_test = shuffle(reviews_test, y_test, random_state=42)\n",
    "\n",
    "# get word embedding model\n",
    "\n",
    "# pretrained:\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "# wv = api.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "# or our own:\n",
    "wv = gensim.models.Word2Vec.load(\"mymodel\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Document similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:56:19,509 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-04-08 14:56:19,551 : INFO : built Dictionary(6268 unique tokens: ['\"a', '\"play', '\"the', '-', 'a']...) from 100 documents (total 22165 corpus positions)\n",
      "2021-04-08 14:56:19,580 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7f563d125a90>\n",
      "2021-04-08 14:56:19,582 : INFO : iterating over columns in dictionary order\n",
      "2021-04-08 14:56:19,588 : INFO : PROGRESS: at 0.02% columns (1 / 6268, 0.015954% density, 0.015954% projected density)\n",
      "2021-04-08 14:56:19,596 : INFO : precomputing L2-norms of word weight vectors\n",
      "2021-04-08 14:56:25,796 : INFO : PROGRESS: at 15.97% columns (1001 / 6268, 0.120628% density, 0.671393% projected density)\n",
      "2021-04-08 14:56:29,934 : INFO : PROGRESS: at 31.92% columns (2001 / 6268, 0.174843% density, 0.513663% projected density)\n",
      "2021-04-08 14:56:33,768 : INFO : PROGRESS: at 47.88% columns (3001 / 6268, 0.219040% density, 0.440127% projected density)\n",
      "2021-04-08 14:56:37,015 : INFO : PROGRESS: at 63.83% columns (4001 / 6268, 0.251885% density, 0.385565% projected density)\n",
      "2021-04-08 14:56:39,792 : INFO : PROGRESS: at 79.79% columns (5001 / 6268, 0.275755% density, 0.341575% projected density)\n",
      "2021-04-08 14:56:42,698 : INFO : PROGRESS: at 95.74% columns (6001 / 6268, 0.297507% density, 0.310034% projected density)\n",
      "2021-04-08 14:56:43,436 : INFO : constructed a sparse term similarity matrix with 0.302139% density\n"
     ]
    }
   ],
   "source": [
    "termsim_index = WordEmbeddingSimilarityIndex(wv)\n",
    "documents = [e.lower().split() for e in reviews_train[:100]]\n",
    "\n",
    "id2word = Dictionary(documents)\n",
    "bow_corpus = [id2word.doc2bow(document) for document in documents]\n",
    "similarity_matrix = SparseTermSimilarityMatrix(termsim_index, id2word)  # construct similarity matrix\n",
    "docsim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''Pulp Fiction may be the single best film ever made, and quite appropriately it is by one of the most \n",
    "creative directors of all time, Quentin Tarantino. This movie is amazing from the beginning definition of pulp to\n",
    "the end credits and boasts one of the best casts ever assembled with the likes of Bruce Willis, Samuel L. Jackson, \n",
    "John Travolta, Uma Thurman, Harvey Keitel, Tim Roth and Christopher Walken. The dialog is surprisingly humorous for\n",
    "this type of film, and I think that's what has made it so successful. Wrongfully denied the many Oscars it was \n",
    "nominated for, Pulp Fiction is by far the best film of the 90s and no Tarantino film has surpassed the quality of\n",
    "this movie (although Kill Bill came close). As far as I'm concerned this is the top film of all-time and definitely \n",
    "deserves a watch if you haven't seen it.\n",
    "'''.lower().split()\n",
    "sims = docsim_index[id2word.doc2bow(query)]                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or let's take a  the first, second, or whatever docuemnt itself\n",
    "\n",
    "docindex = 2\n",
    "\n",
    "sims = docsim_index[id2word.doc2bow(documents[docindex])]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('after watching this movie i was honestly disappointed - not because of the actors, story or directing - i was disappointed by this film advertisements.<br /><br />the trailers were suggesting that the battalion \"have chosen the third way out\" other than surrender or die (polish infos were even misguiding that they had the choice between being killed by own artillery or german guns, they even translated the title wrong as \"misplaced battalion\"). this have tickled the right spot and i bought the movie.<br /><br />the disappointment started when i realized that the third way is to just sit down and count dead bodies followed by sitting down and counting dead bodies... then i began to think \"hey, this story can\\'t be that simple... i bet this clever officer will find some cunning way to save what left of his troops\". well, he didn\\'t, they were just sitting and waiting for something to happen. and so was i.<br /><br />the story was based on real events of world war i, so the writers couldn\\'t make much use of their imagination, but even thought i found this movie really unchallenging and even a little bit boring. and as i wrote in the first place - it isn\\'t fault of actors, writers or director - their marketing people have raised my expectations high above the level that this movie could cope with.',\n",
       " 'After watching this movie I was honestly disappointed - not because of the actors, story or directing - I was disappointed by this film advertisements.<br /><br />The trailers were suggesting that the battalion \"have chosen the third way out\" other than surrender or die (Polish infos were even misguiding that they had the choice between being killed by own artillery or German guns, they even translated the title wrong as \"misplaced battalion\"). This have tickled the right spot and I bought the movie.<br /><br />The disappointment started when I realized that the third way is to just sit down and count dead bodies followed by sitting down and counting dead bodies... Then I began to think \"hey, this story can\\'t be that simple... I bet this clever officer will find some cunning way to save what left of his troops\". Well, he didn\\'t, they were just sitting and waiting for something to happen. And so was I.<br /><br />The story was based on real events of World War I, so the writers couldn\\'t make much use of their imagination, but even thought I found this movie really unchallenging and even a little bit boring. And as I wrote in the first place - it isn\\'t fault of actors, writers or director - their marketing people have raised my expectations high above the level that this movie could cope with.')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check wether everything's ok\n",
    "\" \".join(documents[docindex]), reviews_train[docindex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This review has a similarity of 1.0 with our query:\n",
      "After watching this movie I was honestly disappointed - not because of the actors, story or directing - I was disappointed by this film advertisements.<br /><br />The trailers were suggesting that the battalion \"have chosen the third way out\" other than surrender or die (Polish infos were even misguiding that they had the choice between being killed by own artillery or German guns, they even translated the title wrong as \"misplaced battalion\"). This have tickled the right spot and I bought the movie.<br /><br />The disappointment started when I realized that the third way is to just sit down and count dead bodies followed by sitting down and counting dead bodies... Then I began to think \"hey, this story can't be that simple... I bet this clever officer will find some cunning way to save what left of his troops\". Well, he didn't, they were just sitting and waiting for something to happen. And so was I.<br /><br />The story was based on real events of World War I, so the writers couldn't\n",
      "\n",
      "*************************************************************\n",
      "\n",
      "This review has a similarity of 0.7659871578216553 with our query:\n",
      "I don't think I've ever felt this let down by a film before. After loving Guy Ritchie's two previous films (I don't count Swept Away - he was pussy blind), I was so looking forward to seeing this. <br /><br />The reviews were poor, then again, I don't trust the press anyway. More worrying was the fact that the internet buzz was that this was a bit of a stinker, so it was with some trepidation I handed over my £4.80 yesterday afternoon.<br /><br />I'm not even going to try to explain this film, mainly because I haven't got a clue what was going on and at one point I was honestly close to standing up and asking if it was just me who didn't get it! <br /><br />Unfortunately I think Ritchie seems to have fallen into his wife's trap of taking himself far too seriously.It seems it wasn't good enough for him to make films with good plots, laughs, snappy dialogue and good characters. It's almost as if he had a checklist of films he wanted to rip off, here are some of the ones I noticed:<br /><\n",
      "\n",
      "*************************************************************\n",
      "\n",
      "This review has a similarity of 0.7549089789390564 with our query:\n",
      "I saw this movie at an actual movie theater (probably the $2.00 one) with my cousin and uncle. We were around 11 and 12, I guess, and really into scary movies. I remember being so excited to see it because my cool uncle let us pick the movie (and we probably never got to do that again!) and sooo disappointed afterwards!! Just boring and not scary. The only redeeming thing I can remember was Corky Pigeon from Silver Spoons, and that wasn't all that great, just someone I recognized. I've seen bad movies before and this one has always stuck out in my mind as the worst. This was from what I can recall, one of the most boring, non-scary, waste of our collective $6, and a waste of film. I have read some of the reviews that say it is worth a watch and I say, \"Too each his own\", but I wouldn't even bother. Not even so bad it's good.\n",
      "\n",
      "*************************************************************\n",
      "\n",
      "This review has a similarity of 0.748279869556427 with our query:\n",
      "So I had heard from a few people that this film had brought them to tears in the theater. As I watched it for the first time I was expecting another romantic, tear-jerking Barbra Streisand film; Something like The Way We Were. I was certainly wrong. The chemistry between the two main characters, Esther Hoffman and her John Howard, was nonexistent, making it impossible to get attached to the characters. There wasn't anything romantic about it. Streisand's character fell for an alcoholic drug addict who couldn't sing a single note without making me want to hit the fast-forward button. At one point her character finds her husband in bed with another woman and she forgives him about five minutes later. There's nothing romantic about a deadbeat rock star and a woman who can't seem to realize it until he actually dies. Parts of the movie seemed to drag on and on, and I kept asking myself when it was going to end.<br /><br />The death of John Howard was completely predictable. There was total\n",
      "\n",
      "*************************************************************\n",
      "\n",
      "This review has a similarity of 0.7369922995567322 with our query:\n",
      "I had high expectations of this movie (the title, translated, is \"How We Get Rid of the Others\"). After all, the concept is great: a near future in which the ruling elite has taken the consequence of the right-wing government's constant verbal and legislative persecution of so-called freeloaders and the left wing in general, and decided to just kill off everyone who cannot prove that they're contributing something to the establishment (the establishment being called \"the common good\", but actually meaning the interests of the ruling capitalist ideology).<br /><br />Very cool idea! Ideal for biting satire! Only, this movie completely blows its chance. The satire comes out only in a few scenes and performances of absurdity, but this satire is not sustained; it is neither sharp nor witty. And for an alleged comedy, the movie has nearly no funny scenes. The comedy, I assume, is supposed to be in the absurdity of the situations, but the situations are largely uncomfortable and over-serious,\n",
      "\n",
      "*************************************************************\n",
      "\n",
      "This review has a similarity of 0.7293587923049927 with our query:\n",
      "The only thing I remember about this movie are two things: first, as a twelve year old, even I thought it stunk. Second, it was so bad that when Mad magazine did a parody of it, they quit after the first page, and wrote a disclaimer at the bottom of the page saying that they had completely disavowed it.<br /><br />If you want to see great sophomoric comedies of this period, try Animal House. It's so stupid and vulgar it lowers itself to high art. Another good selection would be Caddyshack, the classic with the late Rodney Dangerfield and Bill Murray before he became annoyingly charming, with great lines like greens keeper Carl Spackler's \"Correct me if I'm wrong Sandy, but if I kill all the golfers they'll lock me up and throw away the key.\"\n",
      "\n",
      "*************************************************************\n",
      "\n",
      "This review has a similarity of 0.7270013093948364 with our query:\n",
      "this, is NOT one of those films it is one of the biggest pieces of tripe I have ever scene, the camera work is trying to be flashy but it really just crap the whole thing looks like the red shoe diaries, but without the sex, the only reason I bought this was I wanted to try out dvd and this was the cheapest one I could find, possibly the worst buy of my life and could have put you off dvd forever, the soundtrack is REALLY tacky and most of the movie is made up of endless repeats of clips from the first two films, why anyone would want to make a movie as awful as this is beyond me, if they had really attempted to make an original movie and failed I would be nicer in this review but they don't they just got the rights to reproduce stuff from the first two and then edit it and repeat it into this film with about maybe under 1 3rd original footage which is about up to the standards of film school students, DO NOT buy this movie. the only entertainment this dvd can offer is if you were to s\n",
      "\n",
      "*************************************************************\n",
      "\n",
      "This review has a similarity of 0.7257627844810486 with our query:\n",
      "I missed the entire season of the show and started watching it on ABC website during the summer of 2007. I am absolutely crazy about the show. I think the entire cast is excellent. It's one of my favorite show ever. I just checked the ABC program lineup for this Fall and did not see it on the schedule. That is really sad. I hope they will bring it back ... maybe they are waiting until Bridget Moynahan has her baby? Or is it only my wishful thinking? <br /><br />I read some of the comments posted about the show and see so many glowing remarks, similar to mine. I certainly hope that ABC will reconsider its decision or hopefully another station will pick it up.\n",
      "\n",
      "*************************************************************\n",
      "\n",
      "This review has a similarity of 0.7240473628044128 with our query:\n",
      "Kevin Spacey is very talented, but unfortunately directing is not his forte. I had high expectations about the film before I rented it and maybe that is why I disliked it so much. I admire Spacey's attempt at making a film that takes place mostly in one small setting, but it's not the attempt that counts. I found the film dull, boring, and stretched out. The acting was nothing spectacular. Gary Sinise has done much better, especially since he is conscious in most of his other films. Skeet Ulrich was disappointing, but this was one of his first films (I did get a kick out of how young and chubby this Scream star looked). The only thing that impressed me about this film was the one shot of the car wreck from above. The center line of the road was perfectly centered and the camera moved on along the line and past the wreck. However, that shot was very \"Usual Suspects\"ish and my guess is Spacey got the idea from that earlier film of his (which is very good mind you). If you want to see a f\n",
      "\n",
      "*************************************************************\n",
      "\n",
      "This review has a similarity of 0.723257303237915 with our query:\n",
      "Johnny and Jeremy are vampires of sorts. Minus the fangs, of course. They're dark, bitter creatures with nothing better to do than to spread their own misery. Through their charms (namely a sharp tongue and a fat wallet, respectively) they seduce desperate souls, who they proceed to torment and victimize. That's more or less the basis of this black comedy, as I understand it.<br /><br />It's not a blend of black humor that I can easily subscribe to, partly because it bothers me to imagine the audience rooting for the sleazy, main character. I did enjoy, however, the sound and the melody of the rapid-fire (and supposedly very witty) remarks. I was very impressed by the cast's strong acting, particularly David Thelis's; only the character of Jeremy seemed too bi-dimensional. The photography and the music, both dramatic and somber, work very well together. <br /><br />What really turns me off about \"Naked\" (and the main reason I'd never recommend it to anyone) is the way it repeatedly see\n",
      "\n",
      "*************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, similarity in sims:\n",
    "    print(f\"This review has a similarity of {similarity} with our query:\")\n",
    "    print(reviews_train[index][:1000])\n",
    "    print(\"\\n*************************************************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.87      0.86     12500\n",
      "         pos       0.87      0.85      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(reviews_train)\n",
    "X_test = vectorizer.transform(reviews_test)\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's discuss\n",
    "\n",
    "- what happened here under the hood?\n",
    "- How many features do we have?\n",
    "- How does X_train \"look\" like?\n",
    "\n",
    "**write your conclusions here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite this into a pipeline (for easier use), and let's use a TfIDF vectorizer instead. This is probably as good as it can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.88      0.88      0.88     12500\n",
      "         pos       0.88      0.88      0.88     12500\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "traditionalpipe = Pipeline([('vectorizer', TfidfVectorizer()),\n",
    "                    ('logreg',LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "traditionalpipe.fit(reviews_train, y_train)\n",
    "y_pred = traditionalpipe.predict(reviews_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's not the topic of today, but once we have such a pipeline, we can use a so-called gridsearch to find the optimal settings. For more info, see https://github.com/damian0604/bdaca/blob/master/12ec/week10/lecture10.pdf**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use embeddings as input instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE THAT YOU KNOW WHICH MODEL YOU ARE WORKING ON - can use either self-trained or pre-trained model\n",
    "\n",
    "# we need to convert `wv` to a slightliy different format:\n",
    "w2vmodel = dict(zip(wv.index2word, wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.90      0.73      0.80     12500\n",
      "         pos       0.77      0.92      0.84     12500\n",
      "\n",
      "    accuracy                           0.82     25000\n",
      "   macro avg       0.83      0.82      0.82     25000\n",
      "weighted avg       0.83      0.82      0.82     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mypipe = Pipeline([('vectorizer', embeddingvectorizer.EmbeddingTfidfVectorizer(w2vmodel, operator='mean')),\n",
    "                    ('svm', \n",
    "                     SGDClassifier(loss='hinge', penalty='l2', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42))])\n",
    "\n",
    "# Generate BOW representation of word counts\n",
    "mypipe.fit(reviews_train, y_train)\n",
    "y_pred = mypipe.predict(reviews_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.83      0.83     12500\n",
      "         pos       0.83      0.83      0.83     12500\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.83      0.83      0.83     25000\n",
      "weighted avg       0.83      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mypipe = Pipeline([('vectorizer', embeddingvectorizer.EmbeddingTfidfVectorizer(w2vmodel, operator='mean')),\n",
    "                    ('logreg', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "# Generate BOW representation of word counts\n",
    "mypipe.fit(reviews_train, y_train)\n",
    "y_pred = mypipe.predict(reviews_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.83      0.83     12500\n",
      "         pos       0.83      0.83      0.83     12500\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.83      0.83      0.83     25000\n",
      "weighted avg       0.83      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mypipe = Pipeline([('vectorizer', embeddingvectorizer.EmbeddingCountVectorizer(w2vmodel, operator='mean')),\n",
    "                    ('logreg', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "# Generate BOW representation of word counts\n",
    "mypipe.fit(reviews_train, y_train)\n",
    "y_pred = mypipe.predict(reviews_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's discuss\n",
    "\n",
    "- what happened here under the hood?\n",
    "- How many features do we have?\n",
    "- How does X_train \"look\" like?\n",
    "\n",
    "**write your conclusions here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There's a reason why the classical approach worked so good that the embedding approach couldn't add anything.\n",
    "\n",
    "- can you see what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_short = reviews_train[:100]\n",
    "reviews_test_short = reviews_test[:100] \n",
    "y_train_short = y_train[:100] \n",
    "y_test_short = y_test[:100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.47      0.60        47\n",
      "         pos       0.66      0.92      0.77        53\n",
      "\n",
      "    accuracy                           0.71       100\n",
      "   macro avg       0.75      0.70      0.69       100\n",
      "weighted avg       0.75      0.71      0.69       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# traditional, short dataset\n",
    "traditionalpipe.fit(reviews_train_short, y_train_short )\n",
    "y_pred_short = traditionalpipe.predict(reviews_test_short)\n",
    "\n",
    "print(metrics.classification_report(y_test_short, y_pred_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.75      0.70      0.73        47\n",
      "         pos       0.75      0.79      0.77        53\n",
      "\n",
      "    accuracy                           0.75       100\n",
      "   macro avg       0.75      0.75      0.75       100\n",
      "weighted avg       0.75      0.75      0.75       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with embeddings, shor tdataset\n",
    "mypipe.fit(reviews_train_short, y_train_short )\n",
    "y_pred_short = mypipe.predict(reviews_test_short)\n",
    "\n",
    "print(metrics.classification_report(y_test_short, y_pred_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**write your conclusions here. Why do you think that the relative advantage turns around with the smaller dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
