{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Monday\n",
    "\n",
    "Train your own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "# tqdm allows you to display progress bars in loops\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# you need to have courseutils.py in the same folder\n",
    "from courseutils import get_review_data\n",
    "\n",
    "import gensim\n",
    "\n",
    "# lets get more output\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get a lot of texts\n",
    "\n",
    "I'll just take the movie reviews here, but you are *very much encouraged* to take your own data. Use any method to get them into a long list (or similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached file reviewdata.pickle.bz2\n"
     ]
    }
   ],
   "source": [
    "train, test, _, _ = get_review_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dataset has two sets of reviews of length 25000 and 25000\n",
      "We merged them into one list of 50000 reviews\n"
     ]
    }
   ],
   "source": [
    "# we just need one list\n",
    "print(f\"The original dataset has two sets of reviews of length {len(train)} and {len(test)}\")\n",
    "train.extend(test)\n",
    "del test\n",
    "print(f\"We merged them into one list of {len(train)} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.',\n",
       " \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Reformat\n",
    "We want to train on sentences, not on whole reviews. We don't need a list of reviews, but a list of sentences.\n",
    "\n",
    "Also, **we only want unique sentences**. It has been shown that this improves the resulting models (and it speeds up training, of course).\n",
    "\n",
    "There are different ways of achieving this, here is one. Some remarks:\n",
    "\n",
    "- tqdm displays a progress bar - it's not strictly necessary\n",
    "- a set is like a list without order, and all items are guaranteed to be unique. You could also use a list, but this is faster. Then, you need to use `uniquesentences = []` and `.append()` instead of `.add()`\n",
    "- we also remove punctuation \n",
    "- depending on whether the texts we want to use our model on later on are lowercased or not, we have to (or not) lowercase here as well. That's a decision to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:19<00:00, 2541.32it/s]\n"
     ]
    }
   ],
   "source": [
    "trans = str.maketrans('', '', string.punctuation) # translation scheme for removing punctuation\n",
    "uniquesentences = set()\n",
    "for review in tqdm(train):\n",
    "    for sentence in sent_tokenize(review):\n",
    "        # remove HTML tags in there\n",
    "        sentence = re.sub(r\"<.*?>\",\" \",sentence)\n",
    "        sentence = sentence.translate(trans) \n",
    "        if sentence not in uniquesentences:\n",
    "            uniquesentences.add(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 516343 unique sentences.\n"
     ]
    }
   ],
   "source": [
    "print(f\"We now have {len(uniquesentences)} unique sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to, we can turn the set into a list and expect it, e.g. like this:\n",
    "# list(uniquesentences)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that unqiesentences can be also a generator that reads from disk (or from elsewhere) for the next step. Hence, it is possible to train models on more sentences than fit in your memory!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the model\n",
    "\n",
    "That's really straightforward in gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not need a list of lists of tokens later on, so let's use a generator instead of a list to save memory\n",
    "# note that we use round parentheses instead of square brackets to achieve this\n",
    "# we do need two generators, though, as we first need to build the vocabulary and later need to train.\n",
    "# If we use a list, we obviously only need once.\n",
    "tokenizedsentences = (sentence.split() for sentence in uniquesentences)\n",
    "tokenizedsentences2 = (sentence.split() for sentence in uniquesentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:42:55,880 : INFO : collecting all words and their counts\n",
      "2021-04-08 12:42:55,880 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-04-08 12:42:55,928 : INFO : PROGRESS: at sentence #10000, processed 215831 words, keeping 20409 word types\n",
      "2021-04-08 12:42:55,979 : INFO : PROGRESS: at sentence #20000, processed 428994 words, keeping 29908 word types\n",
      "2021-04-08 12:42:56,044 : INFO : PROGRESS: at sentence #30000, processed 649955 words, keeping 37572 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started setting up the model at 2021-04-08 12:42:55.879829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:42:56,096 : INFO : PROGRESS: at sentence #40000, processed 870867 words, keeping 43679 word types\n",
      "2021-04-08 12:42:56,148 : INFO : PROGRESS: at sentence #50000, processed 1085511 words, keeping 49110 word types\n",
      "2021-04-08 12:42:56,198 : INFO : PROGRESS: at sentence #60000, processed 1302984 words, keeping 53978 word types\n",
      "2021-04-08 12:42:56,250 : INFO : PROGRESS: at sentence #70000, processed 1524256 words, keeping 58460 word types\n",
      "2021-04-08 12:42:56,300 : INFO : PROGRESS: at sentence #80000, processed 1744797 words, keeping 62623 word types\n",
      "2021-04-08 12:42:56,360 : INFO : PROGRESS: at sentence #90000, processed 1963297 words, keeping 66550 word types\n",
      "2021-04-08 12:42:56,410 : INFO : PROGRESS: at sentence #100000, processed 2178469 words, keeping 70222 word types\n",
      "2021-04-08 12:42:56,461 : INFO : PROGRESS: at sentence #110000, processed 2397220 words, keeping 73787 word types\n",
      "2021-04-08 12:42:56,511 : INFO : PROGRESS: at sentence #120000, processed 2615912 words, keeping 77061 word types\n",
      "2021-04-08 12:42:56,561 : INFO : PROGRESS: at sentence #130000, processed 2832081 words, keeping 80243 word types\n",
      "2021-04-08 12:42:56,613 : INFO : PROGRESS: at sentence #140000, processed 3049597 words, keeping 83439 word types\n",
      "2021-04-08 12:42:56,720 : INFO : PROGRESS: at sentence #150000, processed 3266692 words, keeping 86390 word types\n",
      "2021-04-08 12:42:56,836 : INFO : PROGRESS: at sentence #160000, processed 3485448 words, keeping 89307 word types\n",
      "2021-04-08 12:42:56,920 : INFO : PROGRESS: at sentence #170000, processed 3705586 words, keeping 92252 word types\n",
      "2021-04-08 12:42:57,026 : INFO : PROGRESS: at sentence #180000, processed 3922413 words, keeping 94980 word types\n",
      "2021-04-08 12:42:57,146 : INFO : PROGRESS: at sentence #190000, processed 4139370 words, keeping 97607 word types\n",
      "2021-04-08 12:42:57,240 : INFO : PROGRESS: at sentence #200000, processed 4357836 words, keeping 100208 word types\n",
      "2021-04-08 12:42:57,313 : INFO : PROGRESS: at sentence #210000, processed 4576120 words, keeping 102766 word types\n",
      "2021-04-08 12:42:57,402 : INFO : PROGRESS: at sentence #220000, processed 4795024 words, keeping 105403 word types\n",
      "2021-04-08 12:42:57,462 : INFO : PROGRESS: at sentence #230000, processed 5010843 words, keeping 107964 word types\n",
      "2021-04-08 12:42:57,513 : INFO : PROGRESS: at sentence #240000, processed 5228718 words, keeping 110455 word types\n",
      "2021-04-08 12:42:57,566 : INFO : PROGRESS: at sentence #250000, processed 5448257 words, keeping 112873 word types\n",
      "2021-04-08 12:42:57,617 : INFO : PROGRESS: at sentence #260000, processed 5665706 words, keeping 115305 word types\n",
      "2021-04-08 12:42:57,673 : INFO : PROGRESS: at sentence #270000, processed 5884464 words, keeping 117725 word types\n",
      "2021-04-08 12:42:57,735 : INFO : PROGRESS: at sentence #280000, processed 6101941 words, keeping 120043 word types\n",
      "2021-04-08 12:42:57,801 : INFO : PROGRESS: at sentence #290000, processed 6322364 words, keeping 122426 word types\n",
      "2021-04-08 12:42:57,855 : INFO : PROGRESS: at sentence #300000, processed 6542794 words, keeping 124710 word types\n",
      "2021-04-08 12:42:57,906 : INFO : PROGRESS: at sentence #310000, processed 6756697 words, keeping 126879 word types\n",
      "2021-04-08 12:42:57,963 : INFO : PROGRESS: at sentence #320000, processed 6975601 words, keeping 129035 word types\n",
      "2021-04-08 12:42:58,041 : INFO : PROGRESS: at sentence #330000, processed 7192917 words, keeping 131130 word types\n",
      "2021-04-08 12:42:58,118 : INFO : PROGRESS: at sentence #340000, processed 7411318 words, keeping 133279 word types\n",
      "2021-04-08 12:42:58,177 : INFO : PROGRESS: at sentence #350000, processed 7629272 words, keeping 135450 word types\n",
      "2021-04-08 12:42:58,228 : INFO : PROGRESS: at sentence #360000, processed 7846169 words, keeping 137406 word types\n",
      "2021-04-08 12:42:58,288 : INFO : PROGRESS: at sentence #370000, processed 8065314 words, keeping 139449 word types\n",
      "2021-04-08 12:42:58,347 : INFO : PROGRESS: at sentence #380000, processed 8284244 words, keeping 141373 word types\n",
      "2021-04-08 12:42:58,409 : INFO : PROGRESS: at sentence #390000, processed 8501635 words, keeping 143293 word types\n",
      "2021-04-08 12:42:58,474 : INFO : PROGRESS: at sentence #400000, processed 8721067 words, keeping 145175 word types\n",
      "2021-04-08 12:42:58,545 : INFO : PROGRESS: at sentence #410000, processed 8941547 words, keeping 147121 word types\n",
      "2021-04-08 12:42:58,600 : INFO : PROGRESS: at sentence #420000, processed 9161452 words, keeping 149126 word types\n",
      "2021-04-08 12:42:58,653 : INFO : PROGRESS: at sentence #430000, processed 9381346 words, keeping 151092 word types\n",
      "2021-04-08 12:42:58,705 : INFO : PROGRESS: at sentence #440000, processed 9601952 words, keeping 153101 word types\n",
      "2021-04-08 12:42:58,757 : INFO : PROGRESS: at sentence #450000, processed 9822737 words, keeping 155025 word types\n",
      "2021-04-08 12:42:58,812 : INFO : PROGRESS: at sentence #460000, processed 10042857 words, keeping 156963 word types\n",
      "2021-04-08 12:42:58,880 : INFO : PROGRESS: at sentence #470000, processed 10261530 words, keeping 158875 word types\n",
      "2021-04-08 12:42:58,966 : INFO : PROGRESS: at sentence #480000, processed 10480957 words, keeping 160824 word types\n",
      "2021-04-08 12:42:59,060 : INFO : PROGRESS: at sentence #490000, processed 10697693 words, keeping 162601 word types\n",
      "2021-04-08 12:42:59,147 : INFO : PROGRESS: at sentence #500000, processed 10914005 words, keeping 164367 word types\n",
      "2021-04-08 12:42:59,202 : INFO : PROGRESS: at sentence #510000, processed 11130139 words, keeping 166253 word types\n",
      "2021-04-08 12:42:59,237 : INFO : collected 167386 word types from a corpus of 11272631 raw words and 516343 sentences\n",
      "2021-04-08 12:42:59,238 : INFO : Loading a fresh vocabulary\n",
      "2021-04-08 12:42:59,440 : INFO : effective_min_count=5 retains 42926 unique words (25% of original 167386, drops 124460)\n",
      "2021-04-08 12:42:59,440 : INFO : effective_min_count=5 leaves 11096425 word corpus (98% of original 11272631, drops 176206)\n",
      "2021-04-08 12:42:59,565 : INFO : deleting the raw counts dictionary of 167386 items\n",
      "2021-04-08 12:42:59,571 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2021-04-08 12:42:59,572 : INFO : downsampling leaves estimated 8359192 word corpus (75.3% of prior 11096425)\n",
      "2021-04-08 12:42:59,710 : INFO : estimated required memory for 42926 words and 300 dimensions: 124485400 bytes\n",
      "2021-04-08 12:42:59,711 : INFO : resetting layer weights\n",
      "2021-04-08 12:43:08,691 : INFO : training model with 3 workers on 42926 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training at 2021-04-08 12:43:08.691256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:43:09,715 : INFO : EPOCH 1 - PROGRESS: at 4.23% examples, 347486 words/s, in_qsize 6, out_qsize 0\n",
      "2021-04-08 12:43:10,797 : INFO : EPOCH 1 - PROGRESS: at 8.90% examples, 355144 words/s, in_qsize 6, out_qsize 1\n",
      "2021-04-08 12:43:11,808 : INFO : EPOCH 1 - PROGRESS: at 12.64% examples, 339547 words/s, in_qsize 5, out_qsize 0\n",
      "2021-04-08 12:43:12,809 : INFO : EPOCH 1 - PROGRESS: at 16.12% examples, 328803 words/s, in_qsize 4, out_qsize 1\n",
      "2021-04-08 12:43:13,816 : INFO : EPOCH 1 - PROGRESS: at 20.94% examples, 342361 words/s, in_qsize 5, out_qsize 2\n",
      "2021-04-08 12:43:14,821 : INFO : EPOCH 1 - PROGRESS: at 29.57% examples, 403586 words/s, in_qsize 5, out_qsize 0\n",
      "2021-04-08 12:43:15,829 : INFO : EPOCH 1 - PROGRESS: at 35.58% examples, 417240 words/s, in_qsize 4, out_qsize 1\n",
      "2021-04-08 12:43:16,845 : INFO : EPOCH 1 - PROGRESS: at 41.26% examples, 423454 words/s, in_qsize 4, out_qsize 1\n",
      "2021-04-08 12:43:17,850 : INFO : EPOCH 1 - PROGRESS: at 49.05% examples, 448140 words/s, in_qsize 4, out_qsize 1\n",
      "2021-04-08 12:43:18,861 : INFO : EPOCH 1 - PROGRESS: at 57.63% examples, 474097 words/s, in_qsize 5, out_qsize 0\n",
      "2021-04-08 12:43:19,871 : INFO : EPOCH 1 - PROGRESS: at 65.89% examples, 492939 words/s, in_qsize 4, out_qsize 1\n",
      "2021-04-08 12:43:20,879 : INFO : EPOCH 1 - PROGRESS: at 74.22% examples, 509272 words/s, in_qsize 5, out_qsize 0\n",
      "2021-04-08 12:43:21,903 : INFO : EPOCH 1 - PROGRESS: at 82.59% examples, 523002 words/s, in_qsize 5, out_qsize 2\n",
      "2021-04-08 12:43:22,923 : INFO : EPOCH 1 - PROGRESS: at 91.02% examples, 535496 words/s, in_qsize 5, out_qsize 0\n",
      "2021-04-08 12:43:23,929 : INFO : EPOCH 1 - PROGRESS: at 99.29% examples, 545318 words/s, in_qsize 5, out_qsize 0\n",
      "2021-04-08 12:43:23,994 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-08 12:43:23,999 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-08 12:43:24,002 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-08 12:43:24,003 : INFO : EPOCH - 1 : training on 11272631 raw words (8359139 effective words) took 15.3s, 546719 effective words/s\n",
      "2021-04-08 12:43:24,006 : WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2021-04-08 12:43:24,009 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-08 12:43:24,009 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-08 12:43:24,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-08 12:43:24,011 : INFO : EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-04-08 12:43:24,012 : WARNING : EPOCH - 2 : supplied example count (0) did not equal expected count (516343)\n",
      "2021-04-08 12:43:24,016 : WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2021-04-08 12:43:24,017 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-08 12:43:24,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-08 12:43:24,020 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-08 12:43:24,021 : INFO : EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-04-08 12:43:24,022 : WARNING : EPOCH - 3 : supplied example count (0) did not equal expected count (516343)\n",
      "2021-04-08 12:43:24,027 : WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2021-04-08 12:43:24,028 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-08 12:43:24,028 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-08 12:43:24,029 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-08 12:43:24,029 : INFO : EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-04-08 12:43:24,030 : WARNING : EPOCH - 4 : supplied example count (0) did not equal expected count (516343)\n",
      "2021-04-08 12:43:24,032 : WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2021-04-08 12:43:24,033 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-08 12:43:24,034 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-08 12:43:24,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-08 12:43:24,035 : INFO : EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-04-08 12:43:24,035 : WARNING : EPOCH - 5 : supplied example count (0) did not equal expected count (516343)\n",
      "2021-04-08 12:43:24,035 : INFO : training on a 11272631 raw words (8359139 effective words) took 15.3s, 544798 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training at 2021-04-08 12:43:24.036453\n"
     ]
    }
   ],
   "source": [
    "print(f\"Started setting up the model at {datetime.now()}\")\n",
    "model = gensim.models.Word2Vec(size=300) # we want 300 dimensions\n",
    "model.build_vocab(tokenizedsentences)\n",
    "print(f\"Started training at {datetime.now()}\")\n",
    "model.train(tokenizedsentences2, total_examples=model.corpus_count,  epochs=model.epochs)\n",
    "print(f\"Finished training at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:43:24,040 : INFO : saving Word2Vec object under mymodel, separately None\n",
      "2021-04-08 12:43:24,041 : INFO : storing np array 'vectors' to mymodel.wv.vectors.npy\n",
      "2021-04-08 12:43:24,150 : INFO : not storing attribute vectors_norm\n",
      "2021-04-08 12:43:24,150 : INFO : storing np array 'syn1neg' to mymodel.trainables.syn1neg.npy\n",
      "2021-04-08 12:43:24,263 : INFO : not storing attribute cum_table\n",
      "2021-04-08 12:43:24,313 : INFO : saved mymodel\n"
     ]
    }
   ],
   "source": [
    "model.save(\"mymodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:43:24,318 : INFO : loading Word2Vec object from mymodel\n",
      "2021-04-08 12:43:24,435 : INFO : loading wv recursively from mymodel.wv.* with mmap=None\n",
      "2021-04-08 12:43:24,436 : INFO : loading vectors from mymodel.wv.vectors.npy with mmap=None\n",
      "2021-04-08 12:43:24,484 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-04-08 12:43:24,485 : INFO : loading vocabulary recursively from mymodel.vocabulary.* with mmap=None\n",
      "2021-04-08 12:43:24,485 : INFO : loading trainables recursively from mymodel.trainables.* with mmap=None\n",
      "2021-04-08 12:43:24,486 : INFO : loading syn1neg from mymodel.trainables.syn1neg.npy with mmap=None\n",
      "2021-04-08 12:43:24,510 : INFO : setting ignored attribute cum_table to None\n",
      "2021-04-08 12:43:24,510 : INFO : loaded mymodel\n"
     ]
    }
   ],
   "source": [
    "# and load it again, just to check\n",
    "mymodel = gensim.models.Word2Vec.load(\"mymodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Play with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:43:24,569 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is almost the same as a horse.\n",
      "A dog is almost the same as a cat.\n",
      "A horse is almost the same as a tree.\n",
      "A goldfish is almost the same as a fifi.\n",
      "A lion is almost the same as a luther.\n"
     ]
    }
   ],
   "source": [
    "animals = ['cat', 'dog', 'horse', 'goldfish', 'lion']\n",
    "for animal in animals:\n",
    "    try:\n",
    "        print(f\"A {animal} is almost the same as a {model.wv.most_similar(animal)[0][0]}.\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A director is almost the same as a writer.\n",
      "A actor is almost the same as a actress.\n",
      "A bad is almost the same as a terrible.\n",
      "A good is almost the same as a decent.\n"
     ]
    }
   ],
   "source": [
    "animals = ['director', 'actor', 'bad', 'good']\n",
    "for animal in animals:\n",
    "    try:\n",
    "        print(f\"A {animal} is almost the same as a {model.wv.most_similar(animal)[0][0]}.\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('suspense', 0.7064108848571777),\n",
       " ('gore', 0.6617962121963501),\n",
       " ('adventure', 0.6219613552093506),\n",
       " ('thrills', 0.6181213855743408),\n",
       " ('exciting', 0.6074783802032471),\n",
       " ('slapstick', 0.5946393013000488),\n",
       " ('atmosphere', 0.5907326936721802),\n",
       " ('animation', 0.5855820775032043),\n",
       " ('slasher', 0.5830831527709961),\n",
       " ('excitement', 0.5784242153167725)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.8614335656166077),\n",
       " ('flick', 0.7448533177375793),\n",
       " ('documentary', 0.6746926307678223),\n",
       " ('picture', 0.6668728590011597),\n",
       " ('show', 0.6489842534065247),\n",
       " ('sequel', 0.6386774182319641),\n",
       " ('series', 0.6123355627059937),\n",
       " ('episode', 0.5932444930076599),\n",
       " ('program', 0.5816488265991211),\n",
       " ('garbage', 0.5613850355148315)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Adapt\n",
    "\n",
    "Now it's time to dive into the gensim documentation (online or via `?` / tab completion) to figure out the options you have - e.g., skipgram vs CBOW, dimensions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.Word2Vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
