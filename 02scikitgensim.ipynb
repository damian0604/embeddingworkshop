{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code for second part of the workshop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting everything up\n",
    "\n",
    "Before we start, we import all modules we may want to use later.\n",
    "\n",
    "We also define functions that we may want to use later. In this case, I wrote a function for you that checks whether you have the IMDB dataset on your computer, if not, downloads it, unpacks it, and returns it directly as lists of reviews and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import bz2\n",
    "import urllib.request\n",
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Supervised text classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import eli5\n",
    "\n",
    "# gensim\n",
    "from gensim import downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function I wrote for you to easily get example dataset\n",
    "\n",
    "def get_review_data(filename = \"reviewdata.pickle.bz2\", url = \"http://cssbook.net/d/aclImdb_v1.tar.gz\"):\n",
    "    '''\n",
    "    Checks whether review dataset has already been downloaded.\n",
    "    If not, downloads it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : string\n",
    "        name of cached file\n",
    "    url : string\n",
    "        url of IMDB dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of lists of strings\n",
    "        reviews_train, reviews_test, label_train, label_test\n",
    "    '''\n",
    "\n",
    "    if Path(filename).exists():\n",
    "        print(f\"Using cached file {filename}\")\n",
    "        with bz2.BZ2File(filename, 'r') as f:\n",
    "            reviews_train, reviews_test, label_train, label_test = pickle.load(f)\n",
    "    else:\n",
    "        print(f\"Downloading from {url}\")\n",
    "        fn, _headers = urllib.request.urlretrieve(url, filename=None)\n",
    "        t = tarfile.open(fn, mode=\"r:gz\")\n",
    "        reviews_train, reviews_test, label_train, label_test = [], [], [], []\n",
    "        for file in t.getmembers():\n",
    "            try:\n",
    "                _imdb, dataset, label, _fn = Path(file.name).parts\n",
    "            except ValueError:\n",
    "                # if the Path cannot be parsed, e.g. because it does not consist of exactly four parts, then it is not a part of the dataset but for instance a folder name. Let's skip it then\n",
    "                continue\n",
    "            if dataset == \"train\" and (label=='pos' or label=='neg'):\n",
    "                reviews_train.append(t.extractfile(file).read().decode(\"utf-8\"))\n",
    "                label_train.append(label)\n",
    "            elif dataset == \"test\" and (label=='pos' or label=='neg'):\n",
    "                reviews_test.append(t.extractfile(file).read().decode(\"utf-8\"))\n",
    "                label_test.append(label)\n",
    "        print(f\"Saving {len(label_train)} training and {len(label_test)} test cases to {filename}\")\n",
    "        with bz2.BZ2File(filename, 'w') as f:\n",
    "            pickle.dump((reviews_train, reviews_test, label_train, label_test), f)\n",
    "    return reviews_train, reviews_test, label_train, label_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'another', 'first', 'is', 'text', 'the', 'this', 'yeah']\n",
      "[[0 0 2 1 3 1 1 0]\n",
      " [1 1 0 0 1 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts = ['This is the first text text text first', 'And another text yeah yeah']\n",
    "vec = CountVectorizer(input = texts)\n",
    "vec.fit_transform(texts) \n",
    "\n",
    "# if we want to see what it looks like\n",
    "# DON'T DO THIS WITH LARGE MATRICES!\n",
    "print(vec.get_feature_names())\n",
    "print(vec.transform(texts).todense()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>another</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>text</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>yeah</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This is the first text text text first</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And another text yeah yeah</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        and  another  first  is  text  the  \\\n",
       "This is the first text text text first    0        0      2   1     3    1   \n",
       "And another text yeah yeah                1        1      0   0     1    0   \n",
       "\n",
       "                                        this  yeah  \n",
       "This is the first text text text first     1     0  \n",
       "And another text yeah yeah                 0     2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just for illustration purpose - DON'T DO THIS WITH REAL DATA!\n",
    "pd.DataFrame(vec.transform(texts).todense(), columns=vec.get_feature_names(), index = texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get some data from the IMDB and train a simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached file reviewdata.pickle.bz2\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "reviews_train, reviews_test, y_train, y_test = get_review_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can explore the data here in this cell if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.87      0.86     12500\n",
      "         pos       0.87      0.85      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(reviews_train)\n",
    "X_test = vectorizer.transform(reviews_test)\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "We will use gensim mainly to work with word embeddings. It is also very popular for Topic modelling. If you want to learn more about that, you can have a look here:https://github.com/damian0604/bdaca/blob/master/12ec/week11/lda.ipynb\n",
    "\n",
    "But first, let's just download a small pre-trained word embedding model and play around with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = downloader.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.8798074722290039),\n",
       " ('rabbit', 0.7424426674842834),\n",
       " ('cats', 0.7323004007339478),\n",
       " ('monkey', 0.7288709878921509),\n",
       " ('pet', 0.7190139889717102),\n",
       " ('dogs', 0.7163872718811035),\n",
       " ('mouse', 0.6915250420570374),\n",
       " ('puppy', 0.6800068020820618),\n",
       " ('rat', 0.6641027331352234),\n",
       " ('spider', 0.6501135230064392)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23088  ,  0.28283  ,  0.6318   , -0.59411  , -0.58599  ,\n",
       "        0.63255  ,  0.24402  , -0.14108  ,  0.060815 , -0.7898   ,\n",
       "       -0.29102  ,  0.14287  ,  0.72274  ,  0.20428  ,  0.1407   ,\n",
       "        0.98757  ,  0.52533  ,  0.097456 ,  0.8822   ,  0.51221  ,\n",
       "        0.40204  ,  0.21169  , -0.013109 , -0.71616  ,  0.55387  ,\n",
       "        1.1452   , -0.88044  , -0.50216  , -0.22814  ,  0.023885 ,\n",
       "        0.1072   ,  0.083739 ,  0.55015  ,  0.58479  ,  0.75816  ,\n",
       "        0.45706  , -0.28001  ,  0.25225  ,  0.68965  , -0.60972  ,\n",
       "        0.19578  ,  0.044209 , -0.31136  , -0.68826  , -0.22721  ,\n",
       "        0.46185  , -0.77162  ,  0.10208  ,  0.55636  ,  0.067417 ,\n",
       "       -0.57207  ,  0.23735  ,  0.4717   ,  0.82765  , -0.29263  ,\n",
       "       -1.3422   , -0.099277 ,  0.28139  ,  0.41604  ,  0.10583  ,\n",
       "        0.62203  ,  0.89496  , -0.23446  ,  0.51349  ,  0.99379  ,\n",
       "        1.1846   , -0.16364  ,  0.20653  ,  0.73854  ,  0.24059  ,\n",
       "       -0.96473  ,  0.13481  , -0.0072484,  0.33016  , -0.12365  ,\n",
       "        0.27191  , -0.40951  ,  0.021909 , -0.6069   ,  0.40755  ,\n",
       "        0.19566  , -0.41802  ,  0.18636  , -0.032652 , -0.78571  ,\n",
       "       -0.13847  ,  0.044007 , -0.084423 ,  0.04911  ,  0.24104  ,\n",
       "        0.45273  , -0.18682  ,  0.46182  ,  0.089068 , -0.18185  ,\n",
       "       -0.01523  , -0.7368   , -0.14532  ,  0.15104  , -0.71493  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30817  ,  0.30938  ,  0.52803  , -0.92543  , -0.73671  ,\n",
       "        0.63475  ,  0.44197  ,  0.10262  , -0.09142  , -0.56607  ,\n",
       "       -0.5327   ,  0.2013   ,  0.7704   , -0.13983  ,  0.13727  ,\n",
       "        1.1128   ,  0.89301  , -0.17869  , -0.0019722,  0.57289  ,\n",
       "        0.59479  ,  0.50428  , -0.28991  , -1.3491   ,  0.42756  ,\n",
       "        1.2748   , -1.1613   , -0.41084  ,  0.042804 ,  0.54866  ,\n",
       "        0.18897  ,  0.3759   ,  0.58035  ,  0.66975  ,  0.81156  ,\n",
       "        0.93864  , -0.51005  , -0.070079 ,  0.82819  , -0.35346  ,\n",
       "        0.21086  , -0.24412  , -0.16554  , -0.78358  , -0.48482  ,\n",
       "        0.38968  , -0.86356  , -0.016391 ,  0.31984  , -0.49246  ,\n",
       "       -0.069363 ,  0.018869 , -0.098286 ,  1.3126   , -0.12116  ,\n",
       "       -1.2399   , -0.091429 ,  0.35294  ,  0.64645  ,  0.089642 ,\n",
       "        0.70294  ,  1.1244   ,  0.38639  ,  0.52084  ,  0.98787  ,\n",
       "        0.79952  , -0.34625  ,  0.14095  ,  0.80167  ,  0.20987  ,\n",
       "       -0.86007  , -0.15308  ,  0.074523 ,  0.40816  ,  0.019208 ,\n",
       "        0.51587  , -0.34428  , -0.24525  , -0.77984  ,  0.27425  ,\n",
       "        0.22418  ,  0.20164  ,  0.017431 , -0.014697 , -1.0235   ,\n",
       "       -0.39695  , -0.0056188,  0.30569  ,  0.31748  ,  0.021404 ,\n",
       "        0.11837  , -0.11319  ,  0.42456  ,  0.53405  , -0.16717  ,\n",
       "       -0.27185  , -0.6255   ,  0.12883  ,  0.62529  , -0.52086  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
