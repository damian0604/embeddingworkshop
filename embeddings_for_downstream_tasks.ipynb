{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond counting words: Working with word embeddings\n",
    "\n",
    "Workshop by Damian Trilling\n",
    "\n",
    "This notebook illustrates how we can use embeddings in Machine Learning tasks.\n",
    "\n",
    "As always, we first import neccesary modules. We also get our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# getting data\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import bz2\n",
    "import urllib.request\n",
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# Supervised text classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import eli5\n",
    "from nltk.sentiment import vader\n",
    "\n",
    "# general\n",
    "import numpy as np\n",
    "\n",
    "# word embedding stuff\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function I wrote for you to easily get example dataset\n",
    "\n",
    "def get_review_data(filename = \"reviewdata.pickle.bz2\", url = \"http://cssbook.net/d/aclImdb_v1.tar.gz\"):\n",
    "    '''\n",
    "    Checks whether review dataset has already been downloaded.\n",
    "    If not, downloads it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : string\n",
    "        name of cached file\n",
    "    url : string\n",
    "        url of IMDB dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of lists of strings\n",
    "        reviews_train, reviews_test, label_train, label_test\n",
    "    '''\n",
    "\n",
    "    if Path(filename).exists():\n",
    "        print(f\"Using cached file {filename}\")\n",
    "        with bz2.BZ2File(filename, 'r') as f:\n",
    "            reviews_train, reviews_test, label_train, label_test = pickle.load(f)\n",
    "    else:\n",
    "        print(f\"Downloading from {url}\")\n",
    "        fn, _headers = urllib.request.urlretrieve(url, filename=None)\n",
    "        t = tarfile.open(fn, mode=\"r:gz\")\n",
    "        reviews_train, reviews_test, label_train, label_test = [], [], [], []\n",
    "        for file in t.getmembers():\n",
    "            try:\n",
    "                _imdb, dataset, label, _fn = Path(file.name).parts\n",
    "            except ValueError:\n",
    "                # if the Path cannot be parsed, e.g. because it does not consist of exactly four parts, then it is not a part of the dataset but for instance a folder name. Let's skip it then\n",
    "                continue\n",
    "            if dataset == \"train\" and (label=='pos' or label=='neg'):\n",
    "                reviews_train.append(t.extractfile(file).read().decode(\"utf-8\"))\n",
    "                label_train.append(label)\n",
    "            elif dataset == \"test\" and (label=='pos' or label=='neg'):\n",
    "                reviews_test.append(t.extractfile(file).read().decode(\"utf-8\"))\n",
    "                label_test.append(label)\n",
    "        print(f\"Saving {len(label_train)} training and {len(label_test)} test cases to {filename}\")\n",
    "        with bz2.BZ2File(filename, 'w') as f:\n",
    "            pickle.dump((reviews_train, reviews_test, label_train, label_test), f)\n",
    "    return reviews_train, reviews_test, label_train, label_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached file reviewdata.pickle.bz2\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "reviews_train, reviews_test, y_train, y_test = get_review_data()\n",
    "\n",
    "reviews_train, y_train = shuffle(reviews_train, y_train, random_state=42)\n",
    "reviews_test, y_test = shuffle(reviews_test, y_test, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# get word embedding model\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "#wv = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.87      0.86     12500\n",
      "         pos       0.87      0.85      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(reviews_train)\n",
    "X_test = vectorizer.transform(reviews_test)\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's discuss\n",
    "\n",
    "- what happened here under the hood?\n",
    "- How many features do we have?\n",
    "- How does X_train \"look\" like?\n",
    "\n",
    "**write your conclusions here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use embeddings as input instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_with_embeddings(model, texts, aggfunc=np.mean):\n",
    "    '''Takes a word2vec model and a list or generator of texts as input. Yields the mean embedding for the text'''\n",
    "    for text in texts:\n",
    "        vectors = []\n",
    "        for w in text.split():  # point for improvement: better tokenization here\n",
    "            try:\n",
    "                vectors.append(model[w])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vectors = np.array(vectors)\n",
    "        yield aggfunc(vectors, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.82      0.84      0.83     12500\n",
      "         pos       0.83      0.82      0.83     12500\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.83      0.83      0.83     25000\n",
      "weighted avg       0.83      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = list(vectorize_with_embeddings(wv, reviews_train))\n",
    "X_test = list(vectorize_with_embeddings(wv, reviews_test))\n",
    "\n",
    "logreg2 = LogisticRegression(solver='liblinear')\n",
    "logreg2.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg2.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's discuss\n",
    "\n",
    "- what happened here under the hood?\n",
    "- How many features do we have?\n",
    "- How does X_train \"look\" like?\n",
    "\n",
    "**write your conclusions here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There's a reason why the classical approach worked so good that the embedding approach couldn't add anything.\n",
    "\n",
    "- can you see what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_short = reviews_train[:200]\n",
    "reviews_test_short = reviews_test[:200] \n",
    "y_train_short = y_train[:200] \n",
    "y_test_short = y_test[:200] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.72      0.72      0.72       105\n",
      "         pos       0.69      0.69      0.69        95\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.71      0.71      0.71       200\n",
      "weighted avg       0.71      0.71      0.71       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train_short = vectorizer.fit_transform(reviews_train_short)\n",
    "X_test_short = vectorizer.transform(reviews_test_short)\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train_short, y_train_short)\n",
    "\n",
    "y_pred_short = logreg.predict(X_test_short)\n",
    "\n",
    "print(metrics.classification_report(y_test_short, y_pred_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.78      0.73      0.75       105\n",
      "         pos       0.72      0.77      0.74        95\n",
      "\n",
      "    accuracy                           0.75       200\n",
      "   macro avg       0.75      0.75      0.75       200\n",
      "weighted avg       0.75      0.75      0.75       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_short = list(vectorize_with_embeddings(wv, reviews_train_short, np.sum))\n",
    "X_test_short = list(vectorize_with_embeddings(wv, reviews_test_short, np.sum))\n",
    "\n",
    "logreg2 = LogisticRegression(solver='liblinear')\n",
    "logreg2.fit(X_train_short, y_train_short)\n",
    "\n",
    "y_pred_short = logreg2.predict(X_test_short)\n",
    "\n",
    "print(metrics.classification_report(y_test_short, y_pred_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**write your conclusions here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- slides in between --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "## A simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Embedding, LSTM, GlobalMaxPooling1D\n",
    "from keras.layers import Dense\n",
    "from keras.metrics import Precision, Recall\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 2500\n",
    "np.random.seed(666)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeY(Y):\n",
    "    '''create one-hot (dummies) for output, see also https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "    encode class values as integers\n",
    "    '''\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Y)\n",
    "    encoded_Y = encoder.transform(Y)\n",
    "    dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "    return dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodeY(['aa','bb','aa','cc','aa','cc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(reviews_train)\n",
    "X_test = vectorizer.transform(reviews_test)\n",
    "X_test.sort_indices()\n",
    "X_train.sort_indices()\n",
    "\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "y_train_int = encodeY(y_train)[:,0]\n",
    "y_test_int = encodeY(y_test)[:,0]\n",
    "\n",
    "numberoflabels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 300)               22361700  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 22,362,001\n",
      "Trainable params: 22,362,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "2250/2250 [==============================] - 189s 84ms/step - loss: 0.3891 - accuracy: 0.8331 - precision: 0.8384 - recall: 0.8166 - val_loss: 0.3440 - val_accuracy: 0.8617 - val_precision: 0.8328 - val_recall: 0.9036\n",
      "Epoch 2/5\n",
      "2250/2250 [==============================] - 209s 93ms/step - loss: 0.0873 - accuracy: 0.9701 - precision: 0.9708 - recall: 0.9694 - val_loss: 0.5020 - val_accuracy: 0.8466 - val_precision: 0.8169 - val_recall: 0.8916\n",
      "Epoch 3/5\n",
      "2250/2250 [==============================] - 217s 96ms/step - loss: 0.0187 - accuracy: 0.9947 - precision: 0.9944 - recall: 0.9953 - val_loss: 0.7955 - val_accuracy: 0.8416 - val_precision: 0.8089 - val_recall: 0.8928\n",
      "Epoch 4/5\n",
      "2250/2250 [==============================] - 197s 88ms/step - loss: 0.0039 - accuracy: 0.9996 - precision: 0.9998 - recall: 0.9993 - val_loss: 1.0164 - val_accuracy: 0.8511 - val_precision: 0.8430 - val_recall: 0.8612\n",
      "Epoch 5/5\n",
      "2250/2250 [==============================] - 204s 91ms/step - loss: 6.3622e-04 - accuracy: 0.9998 - precision: 0.9999 - recall: 0.9997 - val_loss: 1.2023 - val_accuracy: 0.8512 - val_precision: 0.8436 - val_recall: 0.8606\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2001 - accuracy: 0.8503 - precision: 0.8434 - recall: 0.8602\n",
      "Accuracy: 0.85, Precision: 0.84, Recall: 0.86\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=input_dim, activation='relu'))\n",
    "#model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "           optimizer='adam', \n",
    "            metrics=['accuracy', Precision(), Recall()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train[:-VALIDATION_SIZE], y_train_int[:-VALIDATION_SIZE],\n",
    "                     epochs=5,\n",
    "                     verbose=True,\n",
    "                     validation_data=(X_test[VALIDATION_SIZE:], y_test_int[VALIDATION_SIZE:]))\n",
    "\n",
    "_, acc, precision, recall = model.evaluate(X_test, y_test_int)\n",
    "print(f\"Accuracy: {acc:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 300)               22361700  \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 22,452,301\n",
      "Trainable params: 22,452,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "2250/2250 [==============================] - 205s 91ms/step - loss: 0.3954 - accuracy: 0.8236 - precision_2: 0.8263 - recall_2: 0.8224 - val_loss: 0.3225 - val_accuracy: 0.8660 - val_precision_2: 0.8593 - val_recall_2: 0.8740\n",
      "Epoch 2/5\n",
      "2250/2250 [==============================] - 192s 85ms/step - loss: 0.0913 - accuracy: 0.9665 - precision_2: 0.9680 - recall_2: 0.9651 - val_loss: 0.5192 - val_accuracy: 0.8485 - val_precision_2: 0.8291 - val_recall_2: 0.8764\n",
      "Epoch 3/5\n",
      "2250/2250 [==============================] - 201s 89ms/step - loss: 0.0184 - accuracy: 0.9941 - precision_2: 0.9944 - recall_2: 0.9938 - val_loss: 1.0019 - val_accuracy: 0.8480 - val_precision_2: 0.8246 - val_recall_2: 0.8824\n",
      "Epoch 4/5\n",
      "2250/2250 [==============================] - 210s 93ms/step - loss: 0.0088 - accuracy: 0.9983 - precision_2: 0.9989 - recall_2: 0.9977 - val_loss: 1.4110 - val_accuracy: 0.8471 - val_precision_2: 0.8529 - val_recall_2: 0.8374\n",
      "Epoch 5/5\n",
      "2250/2250 [==============================] - 221s 98ms/step - loss: 0.0025 - accuracy: 0.9993 - precision_2: 0.9992 - recall_2: 0.9995 - val_loss: 1.9638 - val_accuracy: 0.8482 - val_precision_2: 0.8273 - val_recall_2: 0.8784\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.9630 - accuracy: 0.8484 - precision_2: 0.8279 - recall_2: 0.8797\n",
      "Accuracy: 0.85, Precision: 0.83, Recall: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "           optimizer='adam', \n",
    "            metrics=['accuracy', Precision(), Recall()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train[:-VALIDATION_SIZE], y_train_int[:-VALIDATION_SIZE],\n",
    "                     epochs=5,\n",
    "                     verbose=True,\n",
    "                     validation_data=(X_test[VALIDATION_SIZE:], y_test_int[VALIDATION_SIZE:]))\n",
    "\n",
    "_, acc, precision, recall = model.evaluate(X_test, y_test_int)\n",
    "print(f\"Accuracy: {acc:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STH WRONG FOR EVERYTHING BELOW - ACCURACY= .5 HENCE EFFECTIVELY RANDOM GUESSING GIVEN TWO EQUAL CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, max_df=.9)\n",
    "vectorizer.fit(reviews_train)\n",
    "word2idx = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "tokenize = vectorizer.build_tokenizer()\n",
    "preprocess = vectorizer.build_preprocessor()\n",
    " \n",
    "def to_sequence(tokenizer, preprocessor, index, text):\n",
    "    words = tokenizer(preprocessor(text))\n",
    "    indexes = [index[word] for word in words if word in index]\n",
    "    return indexes\n",
    "\n",
    "X_train_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in reviews_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_SEQ_LENGTH= 1863\n",
      "[27267 27267 27267 ... 13044  3477 15378]\n"
     ]
    }
   ],
   "source": [
    "# Compute the max lenght of a text\n",
    "MAX_SEQ_LENGTH = len(max(X_train_sequences, key=len))\n",
    "print(\"MAX_SEQ_LENGTH=\", MAX_SEQ_LENGTH)\n",
    " \n",
    "N_FEATURES = len(vectorizer.get_feature_names())\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGTH, value=N_FEATURES)\n",
    "print(X_train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1863, 64)          1745152   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1859, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 371, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 23744)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                1519680   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,285,441\n",
      "Trainable params: 3,285,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(vectorizer.get_feature_names()) + 1,\n",
    "                    300,  # Embedding size\n",
    "                    input_length=MAX_SEQ_LENGTH))\n",
    "model.add(Conv1D(300, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=300, activation='relu'))\n",
    "model.add(Dense(units=numberoflabels, activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',  Precision(), Recall()])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "704/704 [==============================] - 98s 138ms/step - loss: 0.0000e+00 - accuracy: 0.5005 - precision_3: 0.5312 - recall_3: 0.0105 - val_loss: 0.0000e+00 - val_accuracy: 0.5012 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 102s 145ms/step - loss: 0.0000e+00 - accuracy: 0.4935 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.5012 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 98s 140ms/step - loss: 0.0000e+00 - accuracy: 0.5063 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.5012 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 97s 138ms/step - loss: 0.0000e+00 - accuracy: 0.5016 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.5012 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 92s 131ms/step - loss: 0.0000e+00 - accuracy: 0.5009 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.5012 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff6c3f28fa0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(X_train_sequences[:-VALIDATION_SIZE], y_train_int[:-VALIDATION_SIZE], \n",
    "          epochs=5, verbose=True,\n",
    "          validation_data=(X_train_sequences[-VALIDATION_SIZE:], y_train_int[-VALIDATION_SIZE:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in reviews_test]\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGTH, value=N_FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 22s 28ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Accuracy: 0.50, Precision: 0.00, Recall: 0.00\n"
     ]
    }
   ],
   "source": [
    "_, acc, precision, recall = model.evaluate(X_test_sequences, y_test_int)\n",
    "print(f\"Accuracy: {acc:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(vectorizer.get_feature_names()) + 1,\n",
    "                    64,  # Embedding size\n",
    "                    input_length=MAX_SEQ_LENGHT))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(units=numberoflabels, activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_sequences[:-VALIDATION_SIZE], y_train[:-VALIDATION_SIZE], \n",
    "          epochs=2, batch_size=128, verbose=1, \n",
    "          validation_data=(X_train_sequences[-VALIDATION_SIZE:], y_train[-VALIDATION_SIZE:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test_sequences, y_test, verbose=1)\n",
    "print(\"Accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = wv.get_keras_embedding(train_embeddings=False)\n",
    "input_dim = (len(X_train_sequences[:-VALIDATION_SIZE]), 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 300)         120000000 \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, None, 300)         270300    \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, None, 150)         135150    \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, None, 75)          33825     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 120,450,826\n",
      "Trainable params: 450,826\n",
      "Non-trainable params: 120,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(300, 3, padding='valid',activation='relu',strides=2))\n",
    "model.add(Conv1D(150, 3, padding='valid',activation='relu',strides=2))\n",
    "model.add(Conv1D(75, 3, padding='valid',activation='relu',strides=2))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(150,activation='sigmoid'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',  Precision(), Recall()])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "704/704 [==============================] - 324s 459ms/step - loss: 0.0000e+00 - accuracy: 0.4996 - precision_5: 0.5161 - recall_5: 0.0276 - val_loss: 0.0000e+00 - val_accuracy: 0.5012 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00\n",
      "Epoch 2/5\n",
      "209/704 [=======>......................] - ETA: 3:50 - loss: 0.0000e+00 - accuracy: 0.5070 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-b81a44df5da7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(X_train_sequences[:-VALIDATION_SIZE], y_train_int[:-VALIDATION_SIZE], \n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           validation_data=(X_train_sequences[-VALIDATION_SIZE:], y_train_int[-VALIDATION_SIZE:]))\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_sequences[:-VALIDATION_SIZE], y_train_int[:-VALIDATION_SIZE], \n",
    "          epochs=5, verbose=True,\n",
    "          validation_data=(X_train_sequences[-VALIDATION_SIZE:], y_train_int[-VALIDATION_SIZE:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, acc, precision, recall = model.evaluate(X_test_sequences, y_test_int)\n",
    "print(f\"Accuracy: {acc:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
